Rong Li:
script for intro:
Sentiment analysis is a subfield of natural language processing (NLP). It is commonly used to gauge the public opinions by analyzing the sentiment of online reviews and make informed decisions.
In our project, we applied Bag of words, word to vector, Global Vectors for Word Representation (GloVe) and fastText methods on Amazon review and drug review datasets.
Our presentation contains three parts: data preprocessing, four models and comparison between them and conclusions.

script for data preprocessing:(make a slides showing before and after)
Data Source: We got the amazon review data from google drive linked in our references. But we only select 100,000 observations due to high computational cost. And the drug review dataset comes from UCI data repository, which contains around 200,000 observations. 
Data process: First we removed any punctuation or special characters from the text and combined the useful columns into single column. On top of that, standardized expression for same words.
For operations like tokenization, stemming, lemmatization, removing stop words are not in data processing step, they are being done when necessary in the model implementation.

script for BOW:
BOW is our first model, we first tokenize the text data by splitting it into individual words. Then for each review, create a vector of the token counts. In this way, we transform the data into matrix where each row is an observation and columns are features. Additionally, we apply two-sample t-test on the matrix we got to select most representative words in each class, then draw wordcloud. As shown, we can significantly see the difference between two classes. Finally, by applying naive bayes on the matrix, we obtained the accuracy of 81% and 75% respectively.


Dai:
The word2vec word embedding that can be used for building our sentiment analysis engine for the sentiment reviews. The pretrained word embedding vector is built using the word2vec model, and it’s based on the Reuter_50_50 dataset.

After obtaining the word embedding, we calculated the review sentences embedding by taking the mean of all the word vectors. 

In our project, we used random forest algorithm to make classification and achieve an accuracy of around 63% on *Amazon Review* and 71% on *Drug Review*

(换页)--------------------------

GloVe combines the global statistics of matrix factorization techniques                                     with local context-based learning   in word2vec. 

Also, rather than using a window to define local context, GloVe constructs an explicit word context or word co-occurrence matrix using statistics across the whole text corpus. 

We use GloVe to obtain word embeddings from our own training corpus, and then we also use the random forest algorithm to build a model to train our classifier. At last, we achieve an accuracy of 73% on *Amazon Review* and 75% on *Drug Review*

Zixing:
fastText is an extension of word2vec for word representation.
Rather than Word2vec and GloVe approaches treat words as the smallest unit to train, fastText breaks words into several n-grams, which can be call as subwords, and the sum of the subwords will be the word embeddings

For the implementation, we used several functions in the fastTextR package to build the model. And we reached accuracy of 86.48% for amazon review dataset and 78.96% for drug dataset. 

And that's the all model we used for our two datasets. 
(------换页----------)
In conclusion, fastText performs better than other models for both datasets, while word2vec has the smallest accuracy results for both datasets. To further improve our word2vec model in the future, we may consider forming our own word embeddings for the dataset when using word2vec method since the original package will make unconvertible sentences into zero vectors. We may also try to split our data into train, validation, and test sets to fine-tune our hyperparameter. 

And that's all for our presentation, thank you for your time!